{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compound-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressing-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./Data/mimic/\"\n",
    "pids = pickle.load(open(os.path.join(DATA_PATH, 'pids.pkl'), 'rb'))\n",
    "vids = pickle.load(open(os.path.join(DATA_PATH, 'vids.pkl'), 'rb'))\n",
    "seqs = pickle.load(open(os.path.join(DATA_PATH, 'seqs.pkl'), 'rb'))\n",
    "types = pickle.load(open(os.path.join(DATA_PATH, 'types.pkl'), 'rb'))\n",
    "rtypes = pickle.load(open(os.path.join(DATA_PATH, 'rtypes.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "filled-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtypes_list = pd.Series(rtypes)\n",
    "diag_codes = rtypes_list[rtypes_list.str.startswith('DIAG_')]\n",
    "diag_codes.reset_index(inplace=True, drop=True)\n",
    "drug_codes = rtypes_list[rtypes_list.str.startswith('DRUG_')]\n",
    "drug_codes.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lyric-riverside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    DIAG_423\n",
       "1    DIAG_511\n",
       "2    DIAG_785\n",
       "3    DIAG_458\n",
       "4    DIAG_311\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "married-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = pd.Series(list(range(len(drug_codes))), index=drug_codes.values).to_dict()\n",
    "diags = pd.Series(list(range(len(diag_codes))), index=diag_codes.values).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outside-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eventlabel2eventid(seqs):\n",
    "    new_seqs, diag_seqs, drug_seqs = [], [], []\n",
    "    for patient in seqs:\n",
    "        events, diag_visits, drug_visits = [], [], []\n",
    "        for visit in patient:\n",
    "            diag_ids, drug_ids = [], []\n",
    "            for event_label in visit:\n",
    "                event_id = rtypes[event_label]\n",
    "                if event_id in diags:\n",
    "                    diag_ids.append(diags[event_id])\n",
    "                else:\n",
    "                    drug_ids.append(drugs[event_id])\n",
    "            diag_visits.append(diag_ids)\n",
    "            drug_visits.append(drug_ids)\n",
    "            vis_events = [diag_ids, drug_ids]\n",
    "            events.append(vis_events)\n",
    "        diag_seqs.append(diag_visits)\n",
    "        drug_seqs.append(drug_visits)\n",
    "        new_seqs.append(events)\n",
    "    return new_seqs, diag_seqs, drug_seqs\n",
    "\n",
    "\n",
    "new_seqs, diag_seqs, drug_seqs = eventlabel2eventid(seqs)\n",
    "len(drug_seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "floppy-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_d = len(diags)\n",
    "C_t = len(drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "difficult-tracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proprietary-enzyme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moved-virgin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_seqs[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "variable-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, code_dim, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(code_dim, emb_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, visit):\n",
    "        return self.relu(self.linear(visit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "appropriate-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNN(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.brnn = nn.GRU(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, emb_visits):\n",
    "        output, _ = self.brnn(emb_visits)\n",
    "        result = output[:, :, : self.hid_dim] + output[:, :, self.hid_dim:]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "italic-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.att = nn.Linear(hid_dim, 1)\n",
    "\n",
    "    def forward(self, com):\n",
    "        return F.softmax(self.att(com), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "passing-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COAM(nn.Module):\n",
    "    def __init__(self, diag_dim=C_d, drug_dim=C_t, emb_dim=256, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.emb_d = Embeddings(diag_dim, emb_dim)\n",
    "        self.emb_t = Embeddings(drug_dim, emb_dim)\n",
    "        self.brnn_d = BRNN(emb_dim, hid_dim)\n",
    "        self.brnn_t = BRNN(emb_dim, hid_dim)\n",
    "        self.com = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.att_a = Attention(hid_dim)\n",
    "        self.att_b = Attention(hid_dim)\n",
    "        self.p = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.output = nn.Linear(hid_dim, diag_dim + drug_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, diag_vis, drug_vis):\n",
    "        d_visits, t_visits = [], []\n",
    "        for i in range(len(diag_vis)):\n",
    "            d_emb = self.emb_d(diag_vis[i])\n",
    "            t_emb = self.emb_t(drug_vis[i])\n",
    "            d_visits.append(d_emb)\n",
    "            t_visits.append(t_emb)\n",
    "        h = self.brnn_d(torch.cat(d_visits).unsqueeze(dim=0))\n",
    "        g = self.brnn_t(torch.cat(t_visits).unsqueeze(dim=0))\n",
    "        com = self.com(torch.cat((h, g), 2))\n",
    "        alpha = self.att_a(com)\n",
    "        beta = self.att_b(com)\n",
    "        h_tilde = torch.mul(beta, h).sum(dim=1)\n",
    "        g_tilde = torch.mul(alpha, g).sum(dim=1)\n",
    "        tilde_cat = torch.cat((h_tilde, g_tilde), 1)\n",
    "        p = self.p(tilde_cat)\n",
    "        result = self.output(p)\n",
    "        result = self.sigmoid(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "level-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COAMa(nn.Module):\n",
    "    def __init__(self, diag_dim=C_d, drug_dim=C_t, emb_dim=256, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.emb_d = Embeddings(diag_dim, emb_dim)\n",
    "        self.emb_t = Embeddings(drug_dim, emb_dim)\n",
    "        self.brnn_d = BRNN(emb_dim, hid_dim)\n",
    "        self.brnn_t = BRNN(emb_dim, hid_dim)\n",
    "        self.com = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.att_a = Attention(hid_dim)\n",
    "        self.att_b = Attention(hid_dim)\n",
    "        self.p = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.output = nn.Linear(hid_dim, diag_dim + drug_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, diag_vis, drug_vis):\n",
    "        d_visits, t_visits = [], []\n",
    "        for i in range(len(diag_vis)):\n",
    "            d_emb = self.emb_d(diag_vis[i])\n",
    "            t_emb = self.emb_t(drug_vis[i])\n",
    "            d_visits.append(d_emb)\n",
    "            t_visits.append(t_emb)\n",
    "        h = self.brnn_d(torch.cat(d_visits).unsqueeze(dim=0))\n",
    "        g = self.brnn_t(torch.cat(t_visits).unsqueeze(dim=0))\n",
    "        alpha = self.att_a(h)\n",
    "        beta = self.att_b(g)\n",
    "        h_tilde = torch.mul(beta, h).sum(dim=1)\n",
    "        g_tilde = torch.mul(alpha, g).sum(dim=1)\n",
    "        tilde_cat = torch.cat((h_tilde, g_tilde), 1)\n",
    "        p = self.p(tilde_cat)\n",
    "        result = self.output(p)\n",
    "        result = self.sigmoid(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "complete-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COAMb(nn.Module):\n",
    "    def __init__(self, diag_dim=C_d, drug_dim=C_t, emb_dim=256, hid_dim=128):\n",
    "        super().__init__()\n",
    "        self.emb_d = Embeddings(diag_dim, emb_dim)\n",
    "        self.emb_t = Embeddings(drug_dim, emb_dim)\n",
    "        self.brnn_d = BRNN(emb_dim, hid_dim)\n",
    "        self.brnn_t = BRNN(emb_dim, hid_dim)\n",
    "        self.com = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.att = Attention(hid_dim)\n",
    "        self.p = nn.Linear(2 * hid_dim, hid_dim)\n",
    "        self.output = nn.Linear(hid_dim, diag_dim + drug_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, diag_vis, drug_vis):\n",
    "        d_visits, t_visits = [], []\n",
    "        for i in range(len(diag_vis)):\n",
    "            d_emb = self.emb_d(diag_vis[i])\n",
    "            t_emb = self.emb_t(drug_vis[i])\n",
    "            d_visits.append(d_emb)\n",
    "            t_visits.append(t_emb)\n",
    "        h = self.brnn_d(torch.cat(d_visits).unsqueeze(dim=0))\n",
    "        g = self.brnn_t(torch.cat(t_visits).unsqueeze(dim=0))\n",
    "        com = self.com(torch.cat((h, g), 2))\n",
    "        att = self.att(com)\n",
    "        h_tilde = torch.mul(att, h).sum(dim=1)\n",
    "        g_tilde = torch.mul(att, g).sum(dim=1)\n",
    "        tilde_cat = torch.cat((h_tilde, g_tilde), 1)\n",
    "        p = self.p(tilde_cat)\n",
    "        result = self.output(p)\n",
    "        result = self.sigmoid(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blind-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "training_index = random.sample(range(len(new_seqs)), k=int(len(new_seqs)*0.8))\n",
    "test_index = list(set(range(len(new_seqs))) - set(training_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "micro-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "advanced-graduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "endangered-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataFormatter(patient_list):\n",
    "    diag_vis, drug_vis = [], []\n",
    "    for visit in patient_list:\n",
    "        diag_codes = torch.zeros((1, C_d))\n",
    "        diag_codes[0, visit[0]] = 1\n",
    "        diag_vis.append(diag_codes)\n",
    "        drug_codes = torch.zeros((1, C_t))\n",
    "        drug_codes[0, visit[1]] = 1\n",
    "        drug_vis.append(drug_codes)\n",
    "    target = torch.cat((diag_vis[-1], drug_vis[-1]), 1)\n",
    "\n",
    "    return diag_vis[:-1], drug_vis[:-1], torch.FloatTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "composite-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def eval_model(model, seqs, data_index):\n",
    "    model.eval()\n",
    "    p, r, f = np.array([]), np.array([]), np.array([])\n",
    "    model.eval()\n",
    "    for p_index in data_index:\n",
    "        patient = seqs[p_index]\n",
    "        for idx, visit in enumerate(patient):\n",
    "            if idx > 0:\n",
    "                diag_vis, drug_vis, y = dataFormatter(patient[:idx+1])\n",
    "                y_hat = model(diag_vis, drug_vis)\n",
    "                y_hat = (y_hat > 0.5).int()\n",
    "                y = y.squeeze()\n",
    "                y_hat = y_hat.squeeze()\n",
    "                new_p, new_r, new_f, _ = precision_recall_fscore_support(y, y_hat, average='binary', zero_division=0)\n",
    "                p, r, f = np.append(p, new_p), np.append(r, new_r), np.append(f, new_f)\n",
    "    \n",
    "    return np.mean(p), np.mean(r), np.mean(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "existing-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_top_k(model, seqs, data_index, k):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    scores = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for p_index in data_index:\n",
    "        patient = seqs[p_index]\n",
    "        for idx, visit in enumerate(patient):\n",
    "            if idx > 0:\n",
    "                diag_vis, drug_vis, y = dataFormatter(patient[:idx+1])\n",
    "                y_hat = model(diag_vis, drug_vis)\n",
    "                y_hat = (y_hat > 0.5).int()\n",
    "                scores = torch.cat((scores, torch.mul(y_hat, y).sum(dim=1)), 0)\n",
    "                y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "                y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \n",
    "    top_k = torch.topk(scores, k).indices\n",
    "    y_true = torch.reshape(y_true[top_k, :], (-1,))\n",
    "    y_pred = torch.reshape(y_pred[top_k, :], (-1,))\n",
    "    result = torch.mul(y_true, y_pred).sum(0) / y_true.sum(dim=0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "corporate-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, optimizer, training_index, seqs, val_index):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for p_index in training_index:\n",
    "#             print('p_index is:', p_index)\n",
    "            loss = 0\n",
    "            patient = seqs[p_index]\n",
    "            for idx, visit in enumerate(patient):\n",
    "                if idx > 0:\n",
    "                    diag_vis, drug_vis, target = dataFormatter(patient[:idx+1])\n",
    "                    pred = model(diag_vis, drug_vis)\n",
    "                    loss += F.binary_cross_entropy_with_logits(pred, target)\n",
    "                    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(training_index)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f = eval_model(model, seqs, val_index)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}'\n",
    "              .format(epoch+1, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "personalized-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "coam = COAM()\n",
    "optimizer = torch.optim.Adam(coam.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exceptional-label",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.142964\n",
      "Epoch: 1 \t Validation p: 0.70, r:0.10, f: 0.17\n",
      "Epoch: 2 \t Training Loss: 1.141942\n",
      "Epoch: 2 \t Validation p: 0.68, r:0.14, f: 0.23\n",
      "Epoch: 3 \t Training Loss: 1.141666\n",
      "Epoch: 3 \t Validation p: 0.71, r:0.14, f: 0.23\n",
      "Epoch: 4 \t Training Loss: 1.141600\n",
      "Epoch: 4 \t Validation p: 0.69, r:0.15, f: 0.24\n",
      "Epoch: 5 \t Training Loss: 1.141556\n",
      "Epoch: 5 \t Validation p: 0.71, r:0.15, f: 0.24\n",
      "Epoch: 6 \t Training Loss: 1.141522\n",
      "Epoch: 6 \t Validation p: 0.72, r:0.14, f: 0.23\n",
      "Epoch: 7 \t Training Loss: 1.141506\n",
      "Epoch: 7 \t Validation p: 0.71, r:0.14, f: 0.23\n",
      "Epoch: 8 \t Training Loss: 1.141467\n",
      "Epoch: 8 \t Validation p: 0.68, r:0.16, f: 0.26\n",
      "Epoch: 9 \t Training Loss: 1.141393\n",
      "Epoch: 9 \t Validation p: 0.68, r:0.18, f: 0.27\n",
      "Epoch: 10 \t Training Loss: 1.141360\n",
      "Epoch: 10 \t Validation p: 0.66, r:0.18, f: 0.27\n",
      "Epoch: 11 \t Training Loss: 1.141315\n",
      "Epoch: 11 \t Validation p: 0.66, r:0.17, f: 0.27\n",
      "Epoch: 12 \t Training Loss: 1.141404\n",
      "Epoch: 12 \t Validation p: 0.66, r:0.17, f: 0.26\n",
      "Epoch: 13 \t Training Loss: 1.141315\n",
      "Epoch: 13 \t Validation p: 0.67, r:0.18, f: 0.28\n",
      "Epoch: 14 \t Training Loss: 1.141249\n",
      "Epoch: 14 \t Validation p: 0.70, r:0.15, f: 0.24\n",
      "Epoch: 15 \t Training Loss: 1.141282\n",
      "Epoch: 15 \t Validation p: 0.66, r:0.19, f: 0.28\n",
      "Epoch: 16 \t Training Loss: 1.141240\n",
      "Epoch: 16 \t Validation p: 0.68, r:0.17, f: 0.27\n",
      "Epoch: 17 \t Training Loss: 1.141200\n",
      "Epoch: 17 \t Validation p: 0.68, r:0.16, f: 0.26\n",
      "Epoch: 18 \t Training Loss: 1.141200\n",
      "Epoch: 18 \t Validation p: 0.68, r:0.19, f: 0.29\n",
      "Epoch: 19 \t Training Loss: 1.141215\n",
      "Epoch: 19 \t Validation p: 0.67, r:0.18, f: 0.27\n",
      "Epoch: 20 \t Training Loss: 1.141173\n",
      "Epoch: 20 \t Validation p: 0.68, r:0.17, f: 0.27\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "train(coam, optimizer, training_index, new_seqs, training_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unique-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation p: 0.67, r:0.17, f: 0.26\n",
      "CPU times: user 7.34 s, sys: 21.2 s, total: 28.6 s\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p, r, f = eval_model(coam, new_seqs, test_index)\n",
    "print('Validation p: {:.2f}, r:{:.2f}, f: {:.2f}'.format(p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "intellectual-agreement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy@5: 0.26\n",
      "Validation Accuracy@10: 0.22\n",
      "Validation Accuracy@15: 0.22\n",
      "Validation Accuracy@20: 0.23\n",
      "CPU times: user 1min 21s, sys: 2min 14s, total: 3min 35s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in [5, 10, 15, 20]:\n",
    "    acc_k = eval_model_top_k(coam, new_seqs, test_index, k)\n",
    "    print('Validation Accuracy@' + str(k) + ': {:.2f}'.format(acc_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "natural-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation p: 0.22\n"
     ]
    }
   ],
   "source": [
    "p = eval_model_top_k(coam, new_seqs, test_index, 10)\n",
    "print('Validation p: {:.2f}'.format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "micro-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "coama = COAMa()\n",
    "optimizera = torch.optim.Adam(coama.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "legitimate-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.143495\n",
      "Epoch: 1 \t Validation p: 0.72, r:0.06, f: 0.10\n",
      "Epoch: 2 \t Training Loss: 1.142636\n",
      "Epoch: 2 \t Validation p: 0.72, r:0.06, f: 0.10\n",
      "Epoch: 3 \t Training Loss: 1.142405\n",
      "Epoch: 3 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 4 \t Training Loss: 1.142299\n",
      "Epoch: 4 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 5 \t Training Loss: 1.142299\n",
      "Epoch: 5 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 6 \t Training Loss: 1.142299\n",
      "Epoch: 6 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 7 \t Training Loss: 1.142299\n",
      "Epoch: 7 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 8 \t Training Loss: 1.142299\n",
      "Epoch: 8 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 9 \t Training Loss: 1.142299\n",
      "Epoch: 9 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 10 \t Training Loss: 1.142299\n",
      "Epoch: 10 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 11 \t Training Loss: 1.142299\n",
      "Epoch: 11 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 12 \t Training Loss: 1.142299\n",
      "Epoch: 12 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 13 \t Training Loss: 1.142299\n",
      "Epoch: 13 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 14 \t Training Loss: 1.142299\n",
      "Epoch: 14 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 15 \t Training Loss: 1.142299\n",
      "Epoch: 15 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 16 \t Training Loss: 1.142299\n",
      "Epoch: 16 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 17 \t Training Loss: 1.142299\n",
      "Epoch: 17 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 18 \t Training Loss: 1.142299\n",
      "Epoch: 18 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 19 \t Training Loss: 1.142299\n",
      "Epoch: 19 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "Epoch: 20 \t Training Loss: 1.142299\n",
      "Epoch: 20 \t Validation p: 0.73, r:0.09, f: 0.15\n",
      "CPU times: user 41min 42s, sys: 1h 50min 54s, total: 2h 32min 37s\n",
      "Wall time: 21min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 20\n",
    "train(coama, optimizera, training_index, new_seqs, training_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "noticed-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation p: 0.72, r:0.08, f: 0.15\n",
      "CPU times: user 7 s, sys: 21.3 s, total: 28.3 s\n",
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p, r, f = eval_model(coama, new_seqs, test_index)\n",
    "print('Validation p: {:.2f}, r:{:.2f}, f: {:.2f}'.format(p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "colonial-terrace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy@5: 0.09\n",
      "Validation Accuracy@10: 0.09\n",
      "Validation Accuracy@15: 0.08\n",
      "Validation Accuracy@20: 0.09\n",
      "CPU times: user 1min 20s, sys: 2min 15s, total: 3min 36s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in [5, 10, 15, 20]:\n",
    "    acc_k = eval_model_top_k(coama, new_seqs, test_index, k)\n",
    "    print('Validation Accuracy@' + str(k) + ': {:.2f}'.format(acc_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "subjective-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "coamb = COAMb()\n",
    "optimizerb = torch.optim.Adam(coamb.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "informal-longitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 1.143332\n",
      "Epoch: 1 \t Validation p: 0.72, r:0.06, f: 0.10\n",
      "Epoch: 2 \t Training Loss: 1.142651\n",
      "Epoch: 2 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 3 \t Training Loss: 1.142749\n",
      "Epoch: 3 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 4 \t Training Loss: 1.142749\n",
      "Epoch: 4 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 5 \t Training Loss: 1.142749\n",
      "Epoch: 5 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 6 \t Training Loss: 1.142749\n",
      "Epoch: 6 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 7 \t Training Loss: 1.142749\n",
      "Epoch: 7 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 8 \t Training Loss: 1.142749\n",
      "Epoch: 8 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 9 \t Training Loss: 1.142749\n",
      "Epoch: 9 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 10 \t Training Loss: 1.142749\n",
      "Epoch: 10 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 11 \t Training Loss: 1.142749\n",
      "Epoch: 11 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 12 \t Training Loss: 1.142749\n",
      "Epoch: 12 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 13 \t Training Loss: 1.142749\n",
      "Epoch: 13 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 14 \t Training Loss: 1.142749\n",
      "Epoch: 14 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 15 \t Training Loss: 1.142749\n",
      "Epoch: 15 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 16 \t Training Loss: 1.142749\n",
      "Epoch: 16 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 17 \t Training Loss: 1.142749\n",
      "Epoch: 17 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 18 \t Training Loss: 1.142749\n",
      "Epoch: 18 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 19 \t Training Loss: 1.142749\n",
      "Epoch: 19 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "Epoch: 20 \t Training Loss: 1.142749\n",
      "Epoch: 20 \t Validation p: 0.67, r:0.09, f: 0.15\n",
      "CPU times: user 42min 55s, sys: 1h 52min 3s, total: 2h 34min 58s\n",
      "Wall time: 21min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 20\n",
    "train(coamb, optimizerb, training_index, new_seqs, training_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "disabled-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation p: 0.65, r:0.09, f: 0.15\n",
      "CPU times: user 7.18 s, sys: 20.7 s, total: 27.9 s\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p, r, f = eval_model(coamb, new_seqs, test_index)\n",
    "print('Validation p: {:.2f}, r:{:.2f}, f: {:.2f}'.format(p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "certified-volume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy@5: 0.07\n",
      "Validation Accuracy@10: 0.09\n",
      "Validation Accuracy@15: 0.09\n",
      "Validation Accuracy@20: 0.09\n",
      "CPU times: user 1min 21s, sys: 2min 9s, total: 3min 30s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for k in [5, 10, 15, 20]:\n",
    "    acc_k = eval_model_top_k(coamb, new_seqs, test_index, k)\n",
    "    print('Validation Accuracy@' + str(k) + ': {:.2f}'.format(acc_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-kenya",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
